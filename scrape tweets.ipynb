{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a75560",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c242ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redditClient():\n",
    "    \"\"\"\n",
    "    Read-only Reddit API access (no login needed).\n",
    "    \"\"\"\n",
    "    redditClient = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password = \"Chanandler1969\",\n",
    "        userName = \"sanchanhart\",\n",
    "        #userAgents = 'client for SNAM2024'\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\"\n",
    "    )\n",
    "    return redditClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6866d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_reddit_posts(subreddit_name, search_query, limit=500):\n",
    "    \"\"\"\n",
    "    Search and collect posts from a subreddit using a keyword.\n",
    "    \"\"\"\n",
    "    reddit = redditClient()\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    posts = []\n",
    "    print(f\"Scraping r/{subreddit_name} for '{search_query}'...\")\n",
    "\n",
    "    for post in subreddit.search(query=search_query, sort='new', limit=limit):\n",
    "        posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"author\": str(post.author),\n",
    "            \"created_utc\": post.created_utc,\n",
    "            \"score\": post.score,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"text\": post.selftext,\n",
    "            \"url\": post.url,\n",
    "            \"id\": post.id\n",
    "        })\n",
    "        time.sleep(0.5)  # Be kind to Reddit API\n",
    "\n",
    "    df = pd.DataFrame(posts)\n",
    "    filename = f\"reddit_{subreddit_name}_{search_query.replace(' ', '_')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Saved {len(df)} posts to {filename}\")\n",
    "    return df\n",
    "\n",
    "# EXAMPLE USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    # Try for Taylor Swift in r/popheads\n",
    "    search_reddit_posts(\"popheads\", \"Taylor Swift\", limit=300)\n",
    "\n",
    "    # Try for Diljit Dosanjh in r/India\n",
    "    search_reddit_posts(\"India\", \"Diljit Dosanjh\", limit=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6638783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Scraper function for a single query + subreddit\n",
    "def scrape_reddit_posts(reddit, query, subreddit_name, limit=300):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    print(f\"üîç Searching r/{subreddit_name} for '{query}'...\")\n",
    "\n",
    "    posts = []\n",
    "    try:\n",
    "        for post in subreddit.search(query=query, sort='new', limit=limit):\n",
    "            posts.append({\n",
    "                \"artist\": query,\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author),\n",
    "                \"created_utc\": datetime.utcfromtimestamp(post.created_utc),\n",
    "                \"score\": post.score,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"text\": post.selftext,\n",
    "                \"url\": post.url,\n",
    "                \"id\": post.id\n",
    "            })\n",
    "            time.sleep(0.3)  # Respect API rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error for {query} in r/{subreddit_name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Collected {len(posts)} posts from r/{subreddit_name} for '{query}'\\n\")\n",
    "    return posts\n",
    "\n",
    "# üß† Master list of artists, subreddits, and queries\n",
    "artists_queries = {\n",
    "    \"Taylor Swift\": [\"Taylor Swift\", \"Eras Tour\", \"1989 Taylors Version\"],\n",
    "    \"Ros√©\": [\"Ros√©\", \"BLACKPINK Ros√©\", \"Ros√© Solo\"],\n",
    "    \"Lisa\": [\"Lisa BLACKPINK\", \"Lalisa\", \"Money Lisa\"],\n",
    "    \"Ariana Grande\": [\"Ariana Grande\", \"ARIANATOR\", \"Ariana concert\"],\n",
    "    \"Jungkook\": [\"Jungkook\", \"BTS Jungkook\", \"Seven Jungkook\"]\n",
    "}\n",
    "\n",
    "subreddits = [\"popheads\", \"kpop\", \"music\", \"BlackPink\", \"Bangtan\", \"ArianaGrande\"]\n",
    "\n",
    "# Run the full scraper\n",
    "def run_scraper():\n",
    "    reddit = redditClient()\n",
    "    all_posts = []\n",
    "\n",
    "    for artist, queries in artists_queries.items():\n",
    "        for query in queries:\n",
    "            for subreddit in subreddits:\n",
    "                posts = scrape_reddit_posts(reddit, query, subreddit, limit=300)\n",
    "                all_posts.extend(posts)\n",
    "\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    df.to_csv(\"reddit_music_virality_data.csv\", index=False)\n",
    "    print(f\"üéâ Done! Total posts collected: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_reddit_posts(query, subreddits, start_date, end_date, max_results=1000):\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    start_epoch = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "    end_epoch = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "    all_posts = []\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        print(f\"Scraping r/{subreddit} for '{query}'...\")\n",
    "        count = 0\n",
    "        before = end_epoch\n",
    "\n",
    "        while count < max_results:\n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"subreddit\": subreddit,\n",
    "                \"after\": start_epoch,\n",
    "                \"before\": before,\n",
    "                \"size\": 100,\n",
    "                \"sort\": \"desc\",\n",
    "                \"sort_type\": \"created_utc\"\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = requests.get(base_url, params=params)\n",
    "                data = response.json().get(\"data\", [])\n",
    "            except:\n",
    "                print(\"‚ùå Error occurred. Retrying...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            for post in data:\n",
    "                all_posts.append({\n",
    "                    \"title\": post.get(\"title\", \"\"),\n",
    "                    \"selftext\": post.get(\"selftext\", \"\"),\n",
    "                    \"subreddit\": post.get(\"subreddit\", \"\"),\n",
    "                    \"author\": post.get(\"author\", \"\"),\n",
    "                    \"score\": post.get(\"score\", 0),\n",
    "                    \"num_comments\": post.get(\"num_comments\", 0),\n",
    "                    \"created_utc\": post.get(\"created_utc\"),\n",
    "                    \"url\": post.get(\"url\", \"\"),\n",
    "                    \"id\": post.get(\"id\", \"\")\n",
    "                })\n",
    "\n",
    "            count += len(data)\n",
    "            before = data[-1][\"created_utc\"]\n",
    "            time.sleep(1)  # to avoid API rate limits\n",
    "\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    df.to_csv(f\"reddit_{query.replace(' ', '_')}_data.csv\", index=False)\n",
    "    print(f\"‚úÖ Done! Collected {len(df)} posts.\")\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "fetch_reddit_posts(\n",
    "    query=\"Cruel Summer\",\n",
    "    subreddits=[\"popheads\", \"Music\", \"TaylorSwift\"],\n",
    "    start_date=\"2023-06-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    max_results=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Initialize Reddit client (using your credentials)\n",
    "def redditClient():\n",
    "    \"\"\"\n",
    "    Read-only Reddit API access (no login needed).\n",
    "    \"\"\"\n",
    "    redditClient = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password=\"Chanandler1969\",\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\",\n",
    "        username=\"sanchanhart\"\n",
    "    )\n",
    "    return redditClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e151ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to scrape posts\n",
    "def scrape_reddit_posts(query, subreddits, limit_per_subreddit=100):\n",
    "    reddit = redditClient()\n",
    "    all_posts = []\n",
    "\n",
    "    for subreddit_name in subreddits:\n",
    "        print(f\"üîç Searching r/{subreddit_name} for '{query}'...\")\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "        try:\n",
    "            for submission in subreddit.search(query, sort=\"new\", limit=limit_per_subreddit):\n",
    "                all_posts.append({\n",
    "                    \"title\": submission.title,\n",
    "                    \"selftext\": submission.selftext,\n",
    "                    \"subreddit\": subreddit_name,\n",
    "                    \"author\": str(submission.author),\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"created_utc\": datetime.utcfromtimestamp(submission.created_utc),\n",
    "                    \"url\": submission.url,\n",
    "                    \"id\": submission.id\n",
    "                })\n",
    "                time.sleep(2)  # Sleep between each post fetch to avoid hitting API rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error while fetching from r/{subreddit_name}: {e}\")\n",
    "        time.sleep(5)  # Sleep between subreddit fetches\n",
    "\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    file_name = f\"reddit_{query.replace(' ', '_')}_posts.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"‚úÖ Done! Collected {len(df)} posts. Saved to {file_name}\")\n",
    "    return df\n",
    "\n",
    "# Step 3: Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    artists = [\"Cruel Summer\"]#, \"Espresso\", \"Vampire\", \"On The Ground\", \"As It Was\", \"Industry Baby\"]\n",
    "    subreddits = [\"popheads\", \"Music\", \"hiphopheads\"]\n",
    "    \n",
    "    for song in artists:\n",
    "        scrape_reddit_posts(query=song, subreddits=subreddits, limit_per_subreddit=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Reddit client\n",
    "def redditClient():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password=\"Chanandler1969\",\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\",\n",
    "        username=\"sanchanhart\"\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "reddit = redditClient()\n",
    "\n",
    "# Artist-song pairs\n",
    "song_artist_pairs = [\n",
    "    (\"Apple\", \"Charli XCX\"),\n",
    "    (\"BIRDS OF A FEATHER\", \"Billie Eilish\"),\n",
    "    (\"Espresso\", \"Sabrina Carpenter\"),\n",
    "    (\"Not Like Us\", \"Kendrick Lamar\"),\n",
    "    (\"Obsessed\", \"Olivia Rodrigo\"),\n",
    "    (\"Too Sweet\", \"Hozier\"),\n",
    "    (\"APT\", \"Rose\"),\n",
    "    (\"FEIN\", \"Travis Scott\"),\n",
    "    (\"Big Dawgs\", \"Hanumankind\"),\n",
    "    (\"Mamushi\", \"Meghan Thee Stallion\")\n",
    "]\n",
    "\n",
    "# Searching across general Reddit\n",
    "subreddits = [\"all\"]\n",
    "\n",
    "# Data collection function\n",
    "def collect_posts_precise(reddit, song, artist, subreddits, limit=200):\n",
    "    collected = []\n",
    "    query = f'\"{song}\" \"{artist}\"'  # Combined query\n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            print(f\"üîç Searching Reddit for '{query}' in r/{sub}...\")\n",
    "            for post in reddit.subreddit(sub).search(query, sort='new', limit=limit):\n",
    "                collected.append({\n",
    "                    \"query\": query,\n",
    "                    \"subreddit\": sub,\n",
    "                    \"title\": post.title,\n",
    "                    \"selftext\": post.selftext,\n",
    "                    \"score\": post.score,\n",
    "                    \"num_comments\": post.num_comments,\n",
    "                    \"created_utc\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"url\": post.url\n",
    "                })\n",
    "                time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error searching '{query}' in subreddit {sub}: {e}\")\n",
    "    return collected\n",
    "\n",
    "# Main scraping loop\n",
    "all_precise_posts = []\n",
    "for song, artist in song_artist_pairs:\n",
    "    print(f\"\\nüé∂ Collecting posts for: {song} by {artist}\")\n",
    "    posts = collect_posts_precise(reddit, song, artist, subreddits)\n",
    "    all_precise_posts.extend(posts)\n",
    "\n",
    "# Save to CSV\n",
    "df_precise = pd.DataFrame(all_precise_posts)\n",
    "df_precise.to_csv(\"reddit_precise_posts.csv\", index=False)\n",
    "print(\"\\n‚úÖ Data collection complete. Saved as reddit_precise_posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Reddit client\n",
    "def redditClient():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password=\"Chanandler1969\",\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\",\n",
    "        username=\"sanchanhart\"\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "reddit = redditClient()\n",
    "\n",
    "# Artist-song pairs\n",
    "song_artist_pairs = [\n",
    "    (\"Apple\", \"Charli XCX\"),\n",
    "    (\"BIRDS OF A FEATHER\", \"Billie Eilish\"),\n",
    "    (\"Espresso\", \"Sabrina Carpenter\"),\n",
    "    (\"Not Like Us\", \"Kendrick Lamar\"),\n",
    "    (\"Obsessed\", \"Olivia Rodrigo\"),\n",
    "    (\"Too Sweet\", \"Hozier\"),\n",
    "    (\"APT\", \"Rose\"),\n",
    "    (\"FEIN\", \"Travis Scott\"),\n",
    "    (\"Big Dawgs\", \"Hanumankind\"),\n",
    "    (\"Mamushi\", \"Meghan Thee Stallion\")\n",
    "]\n",
    "\n",
    "# Subreddits to search\n",
    "subreddits = [\"all\"]\n",
    "\n",
    "# Data collection function\n",
    "def collect_posts_precise(reddit, song, artist, subreddits, limit=200):\n",
    "    collected = []\n",
    "    query = f'\"{song}\" \"{artist}\"'  # Combined query\n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            print(f\"üîç Searching Reddit for '{query}' in r/{sub}...\")\n",
    "            for post in reddit.subreddit(sub).search(query, sort='new', limit=limit):\n",
    "                collected.append({\n",
    "                    \"query\": query,\n",
    "                    \"subreddit\": sub,\n",
    "                    \"title\": post.title,\n",
    "                    \"selftext\": post.selftext,\n",
    "                    \"author\": str(post.author),\n",
    "                    \"id\": post.id,\n",
    "                    \"score\": post.score,\n",
    "                    \"num_comments\": post.num_comments,\n",
    "                    \"created_utc\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"url\": post.url\n",
    "                })\n",
    "                time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error searching '{query}' in r/{sub}: {e}\")\n",
    "    return collected\n",
    "\n",
    "# Main loop for all artist-song pairs\n",
    "all_precise_posts = []\n",
    "\n",
    "for song, artist in song_artist_pairs:\n",
    "    print(f\"\\nüé∂ Collecting posts for: {song} by {artist}\")\n",
    "    posts = collect_posts_precise(reddit, song, artist, subreddits)\n",
    "    all_precise_posts.extend(posts)\n",
    "\n",
    "# Convert and save\n",
    "df = pd.DataFrame(all_precise_posts)\n",
    "df = df[[\"query\", \"title\", \"selftext\", \"subreddit\", \"author\", \"score\", \"num_comments\", \"created_utc\", \"url\", \"id\"]]\n",
    "df.to_csv(\"reddit_precise_posts.csv\", index=False)\n",
    "print(\"\\n‚úÖ Data collection complete. Saved as reddit_precise_posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Reddit client\n",
    "def redditClient():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password=\"Chanandler1969\",\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\",\n",
    "        username=\"sanchanhart\"\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "reddit = redditClient()\n",
    "\n",
    "# Artist-song pairs\n",
    "song_artist_pairs = [\n",
    "    (\"Apple\", \"Charli XCX\"),\n",
    "    (\"BIRDS OF A FEATHER\", \"Billie Eilish\"),\n",
    "    (\"Espresso\", \"Sabrina Carpenter\"),\n",
    "    (\"Not Like Us\", \"Kendrick Lamar\"),\n",
    "    (\"Obsessed\", \"Olivia Rodrigo\"),\n",
    "    (\"Too Sweet\", \"Hozier\"),\n",
    "    (\"APT\", \"Rose\"),\n",
    "    (\"FEIN\", \"Travis Scott\"),\n",
    "    (\"Big Dawgs\", \"Hanumankind\"),\n",
    "    (\"Mamushi\", \"Meghan Thee Stallion\")\n",
    "]\n",
    "\n",
    "# Subreddits to search\n",
    "subreddits = [\"all\"]\n",
    "\n",
    "# Data collection function\n",
    "def collect_posts_precise(reddit, song, artist, subreddits, limit=200):\n",
    "    collected = []\n",
    "    query = f'\"{song}\" \"{artist}\"'\n",
    "    \n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            print(f\"üîç Searching Reddit for '{query}' in r/{sub}...\")\n",
    "            for post in reddit.subreddit(sub).search(query, sort='new', limit=limit):\n",
    "\n",
    "                # ‚úÖ Filter: Only save posts that are actual submissions with meaningful content\n",
    "                if not post.title or len(post.title.strip()) < 5:\n",
    "                    continue  # skip blank titles or short junk\n",
    "                if post.selftext and \"http\" in post.selftext.lower() and len(post.selftext.strip()) < 50:\n",
    "                    continue  # skip spam links\n",
    "                if post.author is None or str(post.author).lower() == \"automoderator\":\n",
    "                    continue  # skip bot/mod posts\n",
    "\n",
    "                try:\n",
    "                    collected.append({\n",
    "                        \"query\": query,\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": post.title.strip(),\n",
    "                        \"selftext\": post.selftext.strip() if post.selftext else \"\",\n",
    "                        \"author\": str(post.author),\n",
    "                        \"id\": post.id,\n",
    "                        \"score\": post.score,\n",
    "                        \"num_comments\": post.num_comments,\n",
    "                        \"created_utc\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        \"url\": post.url\n",
    "                    })\n",
    "                except Exception as inner_error:\n",
    "                    print(f\"‚ö†Ô∏è Skipped malformed post: {inner_error}\")\n",
    "\n",
    "                time.sleep(0.2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching '{query}' in subreddit {sub}: {e}\")\n",
    "\n",
    "    return collected\n",
    "\n",
    "\n",
    "# Main loop for all artist-song pairs\n",
    "all_precise_posts = []\n",
    "\n",
    "for song, artist in song_artist_pairs:\n",
    "    print(f\"\\nüé∂ Collecting posts for: {song} by {artist}\")\n",
    "    posts = collect_posts_precise(reddit, song, artist, subreddits)\n",
    "    all_precise_posts.extend(posts)\n",
    "\n",
    "# Convert and save\n",
    "df = pd.DataFrame(all_precise_posts)\n",
    "df = df[[\"query\", \"title\", \"selftext\", \"subreddit\", \"author\", \"score\", \"num_comments\", \"created_utc\", \"url\", \"id\"]]\n",
    "df.to_csv(\"reddit_precise_posts_2.csv\", index=False)\n",
    "print(\"\\n‚úÖ Data collection complete. Saved as reddit_precise_posts_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60071dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Setup Reddit client\n",
    "def redditClient():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password=\"Chanandler1969\",\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\",\n",
    "        username=\"sanchanhart\"\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "reddit = redditClient()\n",
    "\n",
    "# Step 2: Define artists and songs\n",
    "song_artist_pairs = [\n",
    "    (\"Apple\", \"Charli XCX\"),\n",
    "    (\"BIRDS OF A FEATHER\", \"Billie Eilish\"),\n",
    "    (\"Espresso\", \"Sabrina Carpenter\"),\n",
    "    (\"Not Like Us\", \"Kendrick Lamar\"),\n",
    "    (\"Obsessed\", \"Olivia Rodrigo\"),\n",
    "    (\"Too Sweet\", \"Hozier\"),\n",
    "    (\"APT\", \"Rose\"),\n",
    "    (\"FEIN\", \"Travis Scott\"),\n",
    "    (\"Big Dawgs\", \"Hanumankind\"),\n",
    "    (\"Mamushi\", \"Meghan Thee Stallion\")\n",
    "]\n",
    "\n",
    "subreddits = [\"all\"]\n",
    "\n",
    "# Step 3: Collect comments from posts that match song + artist\n",
    "def scrape_comments_from_matching_posts(reddit, song, artist, subreddits, limit=50, max_comments=100):\n",
    "    all_comments = []\n",
    "    query = f'\"{song}\" \"{artist}\"'\n",
    "\n",
    "    for sub in subreddits:\n",
    "        print(f\"\\nüîç Searching for posts: {query} in r/{sub}\")\n",
    "        try:\n",
    "            for post in reddit.subreddit(sub).search(query, sort='new', limit=limit):\n",
    "                try:\n",
    "                    print(f\"üßµ Scraping comments from post: {post.title[:60]}\")\n",
    "\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    count = 0\n",
    "                    for comment in post.comments.list():\n",
    "                        if comment.body and comment.author:\n",
    "                            all_comments.append({\n",
    "                                \"song\": song,\n",
    "                                \"artist\": artist,\n",
    "                                \"subreddit\": sub,\n",
    "                                \"post_id\": post.id,\n",
    "                                \"post_title\": post.title,\n",
    "                                \"comment_id\": comment.id,\n",
    "                                \"author\": str(comment.author),\n",
    "                                \"body\": comment.body.strip(),\n",
    "                                \"score\": comment.score,\n",
    "                                \"created_utc\": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            })\n",
    "                            count += 1\n",
    "                        if count >= max_comments:\n",
    "                            break\n",
    "                    time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Comment scraping error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Post search error in r/{sub}: {e}\")\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# Step 4: Scrape all comments\n",
    "final_comments = []\n",
    "\n",
    "for song, artist in song_artist_pairs:\n",
    "    comments = scrape_comments_from_matching_posts(\n",
    "        reddit, song, artist, subreddits, limit=75, max_comments=80\n",
    "    )\n",
    "    final_comments.extend(comments)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_comments = pd.DataFrame(final_comments)\n",
    "df_comments.to_csv(\"reddit_song_artist_comments.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Scraping complete. Total comments collected: {len(df_comments)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4923c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Setup Reddit client\n",
    "def redditClient():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"EzHVeRw_MAcYTVklYzLs0g\",\n",
    "        client_secret=\"ZZG-6PzEwN-cozi4jrV9ArzjOL_KJg\",\n",
    "        password=\"Chanandler1969\",\n",
    "        user_agent=\"script:tennis.goat.analysis:v1.0 (by u/sanchanhart)\",\n",
    "        username=\"sanchanhart\"\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "reddit = redditClient()\n",
    "\n",
    "# Step 2: Target song and artist\n",
    "song = \"Apple\"\n",
    "artist = \"Charli XCX\"\n",
    "query = f'\"{song}\" AND \"{artist}\"'  # Improved for keyword match\n",
    "\n",
    "# Step 3: Relevant subreddits only\n",
    "subreddits = [\"charlixcx\"]\n",
    "\n",
    "# Step 4: Scrape comments from relevant posts\n",
    "def scrape_comments_from_query(reddit, query, subreddits, post_limit=100, max_comments_per_post=50):\n",
    "    comments = []\n",
    "\n",
    "    for sub in subreddits:\n",
    "        print(f\"\\nüîç Searching r/{sub} for: {query}\")\n",
    "        try:\n",
    "            for post in reddit.subreddit(sub).search(query, sort=\"top\", limit=post_limit):\n",
    "                try:\n",
    "                    print(f\"üßµ Post: {post.title[:60]} | {post.score} upvotes\")\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    count = 0\n",
    "\n",
    "                    for comment in post.comments.list():\n",
    "                        if comment.body and comment.author and len(comment.body.strip()) > 5:\n",
    "                            comments.append({\n",
    "                                \"song\": song,\n",
    "                                \"artist\": artist,\n",
    "                                \"subreddit\": sub,\n",
    "                                \"post_id\": post.id,\n",
    "                                \"post_title\": post.title,\n",
    "                                \"comment_id\": comment.id,\n",
    "                                \"author\": str(comment.author),\n",
    "                                \"body\": comment.body.strip(),\n",
    "                                \"score\": comment.score,\n",
    "                                \"created_utc\": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            })\n",
    "                            count += 1\n",
    "                        if count >= max_comments_per_post:\n",
    "                            break\n",
    "\n",
    "                    time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to scrape comments: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search failed in r/{sub}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(comments)\n",
    "\n",
    "# Step 5: Scrape and save\n",
    "df_comments = scrape_comments_from_query(reddit, query, subreddits, post_limit=100, max_comments_per_post=50)\n",
    "df_comments.to_csv(\"charli_xcx_apple_comments.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Done! Total comments collected: {len(df_comments)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23dc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-api-python-client pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "API_KEY = 'AIzaSyAtB-fQhUp6v7sC4IBdEhnRlvc5239J-Dw'\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def search_video(song, artist):\n",
    "    query = f\"{song} {artist} official video\"\n",
    "    request = youtube.search().list(\n",
    "        q=query,\n",
    "        part=\"snippet\",\n",
    "        maxResults=1,\n",
    "        type=\"video\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    video_id = response[\"items\"][0][\"id\"][\"videoId\"]\n",
    "    return video_id\n",
    "\n",
    "def get_video_metadata(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    item = response[\"items\"][0]\n",
    "    metadata = {\n",
    "        \"video_id\": video_id,\n",
    "        \"title\": item[\"snippet\"][\"title\"],\n",
    "        \"channel\": item[\"snippet\"][\"channelTitle\"],\n",
    "        \"published\": item[\"snippet\"][\"publishedAt\"],\n",
    "        \"description\": item[\"snippet\"][\"description\"],\n",
    "        \"views\": item[\"statistics\"].get(\"viewCount\"),\n",
    "        \"likes\": item[\"statistics\"].get(\"likeCount\"),\n",
    "        \"comments\": item[\"statistics\"].get(\"commentCount\")\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "def get_comments(video_id, max_comments=150):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    while request and len(comments) < max_comments:\n",
    "        for item in response[\"items\"]:\n",
    "            snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comments.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"author\": snippet[\"authorDisplayName\"],\n",
    "                \"comment\": snippet[\"textDisplay\"],\n",
    "                \"likes\": snippet[\"likeCount\"],\n",
    "                \"published\": snippet[\"publishedAt\"]\n",
    "            })\n",
    "        if \"nextPageToken\" in response and len(comments) < max_comments:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=response[\"nextPageToken\"],\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        else:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "    return pd.DataFrame(comments)\n",
    "\n",
    "# List of songs and artists\n",
    "song_artist_pairs = [\n",
    "    (\"Apple\", \"Charli XCX\"),\n",
    "    (\"BIRDS OF A FEATHER\", \"Billie Eilish\"),\n",
    "    (\"Espresso\", \"Sabrina Carpenter\")\n",
    "]\n",
    "\n",
    "for song, artist in song_artist_pairs:\n",
    "    try:\n",
    "        print(f\"üéµ Processing: {song} by {artist}\")\n",
    "        video_id = search_video(song, artist)\n",
    "        metadata = get_video_metadata(video_id)\n",
    "        comments_df = get_comments(video_id, max_comments=150)\n",
    "        pd.DataFrame([metadata]).to_csv(f\"{artist}_{song}_metadata.csv\", index=False)\n",
    "        comments_df.to_csv(f\"{artist}_{song}_comments.csv\", index=False)\n",
    "        print(f\"‚úÖ Done: {len(comments_df)} comments scraped for '{song}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {song} by {artist}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9ac66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28a6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "API_KEY = 'AIzaSyAtB-fQhUp6v7sC4IBdEhnRlvc5239J-Dw'\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def search_video(song, artist):\n",
    "    query = f\"{song} {artist} official video\"\n",
    "    request = youtube.search().list(\n",
    "        q=query,\n",
    "        part=\"snippet\",\n",
    "        maxResults=1,\n",
    "        type=\"video\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    video_id = response[\"items\"][0][\"id\"][\"videoId\"]\n",
    "    return video_id\n",
    "\n",
    "def get_video_metadata(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    item = response[\"items\"][0]\n",
    "    return {\n",
    "        \"video_id\": video_id,\n",
    "        \"title\": item[\"snippet\"][\"title\"],\n",
    "        \"channel\": item[\"snippet\"][\"channelTitle\"],\n",
    "        \"published\": item[\"snippet\"][\"publishedAt\"],\n",
    "        \"description\": item[\"snippet\"][\"description\"],\n",
    "        \"views\": item[\"statistics\"].get(\"viewCount\"),\n",
    "        \"likes\": item[\"statistics\"].get(\"likeCount\"),\n",
    "        \"comments\": item[\"statistics\"].get(\"commentCount\")\n",
    "    }\n",
    "\n",
    "def get_comments(video_id, max_comments=1500):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    while request and len(comments) < max_comments:\n",
    "        for item in response[\"items\"]:\n",
    "            snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comments.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"author\": snippet[\"authorDisplayName\"],\n",
    "                \"comment\": snippet[\"textDisplay\"],\n",
    "                \"likes\": snippet[\"likeCount\"],\n",
    "                \"published\": snippet[\"publishedAt\"]\n",
    "            })\n",
    "        if \"nextPageToken\" in response and len(comments) < max_comments:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=response[\"nextPageToken\"],\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        else:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "    return pd.DataFrame(comments)\n",
    "\n",
    "def combine_metadata_and_comments(song, artist):\n",
    "    try:\n",
    "        video_id = search_video(song, artist)\n",
    "        metadata = get_video_metadata(video_id)\n",
    "        comments_df = get_comments(video_id, max_comments=1500)\n",
    "\n",
    "        for key, value in metadata.items():\n",
    "            comments_df[key] = value\n",
    "\n",
    "        return comments_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {song} by {artist}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2159fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé∂ Fetching data for Apple by Charli xcx...\n",
      "üé∂ Fetching data for BIRDS OF A FEATHER by Billie Eilish...\n",
      "üé∂ Fetching data for Espresso by Sabrina Carpenter...\n",
      "üé∂ Fetching data for Not Like Us by Kendrick Lamar...\n",
      "üé∂ Fetching data for Obsessed by Olivia Rodrigo...\n",
      "üé∂ Fetching data for Too Sweet by Hozier...\n",
      "üé∂ Fetching data for APT by Rose...\n",
      "üé∂ Fetching data for FEIN by Travis Scott...\n",
      "üé∂ Fetching data for Big Dawgs by Hanumankind...\n",
      "üé∂ Fetching data for Mamushi by Meghan Thee Stallion...\n",
      "‚úÖ Scraping completed and saved to 'youtube_song_comments_data_1.csv'\n"
     ]
    }
   ],
   "source": [
    "songs = [\n",
    "    (\"Apple\", \"Charli xcx\"),\n",
    "    (\"BIRDS OF A FEATHER\", \"Billie Eilish\"),\n",
    "    (\"Espresso\", \"Sabrina Carpenter\"),\n",
    "    (\"Not Like Us\", \"Kendrick Lamar\"),\n",
    "    (\"Obsessed\", \"Olivia Rodrigo\"),\n",
    "    (\"Too Sweet\", \"Hozier\"),\n",
    "    (\"APT\", \"Rose\"),\n",
    "    (\"FEIN\", \"Travis Scott\"),\n",
    "    (\"Big Dawgs\", \"Hanumankind\"),\n",
    "    (\"Mamushi\", \"Meghan Thee Stallion\")\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for song, artist in songs:\n",
    "    print(f\"üé∂ Fetching data for {song} by {artist}...\")\n",
    "    df = combine_metadata_and_comments(song, artist)\n",
    "    all_data.append(df)\n",
    "\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df.to_csv(\"youtube_song_comments_data_1.csv\", index=False)\n",
    "print(\"‚úÖ Scraping completed and saved to 'youtube_song_comments_data_1.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bec6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e858362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
